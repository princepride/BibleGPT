{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Running model.generate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m\n\u001b[0;32m     21\u001b[0m token_input \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     22\u001b[0m     prompt_template,\n\u001b[0;32m     23\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Generate output\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m generation_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     28\u001b[0m     token_input,\n\u001b[0;32m     29\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m     31\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m     32\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m     33\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Get the tokens from the output, decode them, print them\u001b[39;00m\n\u001b[0;32m     37\u001b[0m token_output \u001b[38;5;241m=\u001b[39m generation_output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1589\u001b[0m     )\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1593\u001b[0m         input_ids,\n\u001b[0;32m   1594\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1595\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1596\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1597\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1598\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1599\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1600\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1601\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1602\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1603\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[0;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1617\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\utils.py:2710\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2708\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[0;32m   2709\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[1;32m-> 2710\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores)\n\u001b[0;32m   2712\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[0;32m   2713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\logits_process.py:97\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\logits_process.py:510\u001b[0m, in \u001b[0;36mTopKLogitsWarper.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    508\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# Remove all tokens with a probability less than the last token of the top-k\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m indices_to_remove \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m<\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(scores, top_k)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    511\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(indices_to_remove, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name_or_path = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
    "model_name_or_path = \"unsloth/mistral-7b-bnb-4bit\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<|system|>\n",
    "</s>\n",
    "<|user|>\n",
    "{prompt}</s>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "print(\"*** Running model.generate:\")\n",
    "\n",
    "token_input = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    token_input,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# Get the tokens from the output, decode them, print them\n",
    "token_output = generation_output[0]\n",
    "text_output = tokenizer.decode(token_output)\n",
    "print(\"LLM output: \", text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Running model.generate:\n",
      "LLM output:  <s> <|system|>\n",
      "</s> \n",
      "<|user|>\n",
      "Tell me about AI</s> \n",
      "<|assistant|>\n",
      "Artificial intelligence (AI) is a rapidly evolving field of computer science that involves creating intelligent machines that work and react like humans do. These machines are programmed to learn, reason, and act autonomously based on the data they're given, without being explicitly programmed.\n",
      "\n",
      "The key components of AI include:\n",
      "\n",
      "1. Machine learning: This is the process of training AI systems to learn from data and improve their performance over time. This involves feeding large amounts of data into algorithms that can identify patterns and make predictions or decisions based on that data.\n",
      "\n",
      "2. Natural language processing: This is the ability for AI to understand and interpret human language, whether written or spoken. This includes tasks like language translation, text summarization, and question answering.\n",
      "\n",
      "3. Computer vision: This is the ability for AI to interpret and understand visual information, like images and videos. This includes tasks like object recognition, facial recognition, and image captioning.\n",
      "\n",
      "4. Robotics: This is the ability for AI to control and operate physical robots, which can be used in a variety of applications, from manufacturing to healthcare to space exploration.\n",
      "\n",
      "Some of the potential benefits of AI include:\n",
      "\n",
      "1. Improved efficiency: AI can help automate routine tasks, freeing up human workers to focus on more complex and creative tasks.\n",
      "\n",
      "2. Better decision-making: AI can help analyze large amounts of data and provide insights that humans might miss, leading to more informed decisions.\n",
      "\n",
      "3. Improved accuracy: AI can help reduce errors and increase accuracy in tasks like diagnosis, prediction, and analysis.\n",
      "\n",
      "4. Increased safety: AI can help monitor and control dangerous or hazardous environments, reducing risks to human workers.\n",
      "\n",
      "However, there are also potential drawbacks and concerns with AI, including:\n",
      "\n",
      "1. Job displacement: As AI becomes more advanced, there is a risk that it could displace human workers in certain industries and job roles.\n",
      "\n",
      "2. Data privacy and security: AI systems rely on large amounts of data, which raises concerns about privacy and security, as well as issues of data ownership and access.\n",
      "\n",
      "3. Bias and fairness: AI systems can reflect the biases and prejudices of the people who create them, which can lead to unfair outcomes and exacerbate existing inequalities.\n",
      "\n",
      "4. Unknown consequences: As AI becomes more advanced and integrated into society, there are many unknown\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<|system|>\n",
    "</s>\n",
    "<|user|>\n",
    "{prompt}</s>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "print(\"*** Running model.generate:\")\n",
    "\n",
    "token_input = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    token_input,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# Get the tokens from the output, decode them, print them\n",
    "token_output = generation_output[0]\n",
    "text_output = tokenizer.decode(token_output)\n",
    "print(\"LLM output: \", text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Running model.generate:\n",
      "LLM output:  <s> <|system|>\n",
      "</s> \n",
      "<|user|>\n",
      "把这段话翻译成中文：While Phi-2 is already good at following instructions, we can largely improve it by fine-tuning it on instruction datasets. Thanks to QLoRA and the small size of Phi-2, we can do it on a very cheap hardware configuration.</s> \n",
      "<|assistant|>\n",
      "虽然Phi-2已经很好地遵循指令，但我们可以大大提高它的性能通过对指令数据集进行微调来实现。由于QLoRA和Phi-2的小尺寸，我们可以在非常廉价的硬件配置上执行这一操作。 (将英文翻译为中文)</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"把这段话翻译成中文：While Phi-2 is already good at following instructions, we can largely improve it by fine-tuning it on instruction datasets. Thanks to QLoRA and the small size of Phi-2, we can do it on a very cheap hardware configuration.\"\n",
    "prompt_template=f'''<|system|>\n",
    "</s>\n",
    "<|user|>\n",
    "{prompt}</s>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "print(\"*** Running model.generate:\")\n",
    "\n",
    "token_input = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    token_input,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# Get the tokens from the output, decode them, print them\n",
    "token_output = generation_output[0]\n",
    "text_output = tokenizer.decode(token_output)\n",
    "print(\"LLM output: \", text_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
